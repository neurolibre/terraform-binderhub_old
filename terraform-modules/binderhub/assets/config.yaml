# Additionnal optimizations
#https://alan-turing-institute.github.io/hub23-deploy/advanced/optimising-jupyterhub.html#labelling-nodes-for-core-purpose
#https://github.com/alan-turing-institute/hub23-deploy/blob/main/deploy/prod.yaml#L56

# add additionnal buttons https://binderhub.readthedocs.io/en/latest/cors.html#adjusting-binderhub-config-to-enable-cors
cors: &cors
  allowOrigin: '*'

jupyterhub:
  custom:
    cors: *cors      
  ingress:
    enabled: true
    hosts:
      - binder.conp.cloud
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      cert-manager.io/issuer: letsencrypt-production
    https:
      enabled: true
      type: nginx
    config:
      # Allow POSTs of upto 64MB, for large notebook support.
      proxy-body-size: 64m
    tls:
      - secretName: binder-conp-cloud-tls
        hosts:
          - binder.conp.cloud
  hub:
    baseUrl: /jupyter/
    image:
      name: conpdev/hub-image
      tag: v1.9
    fsGid: 999
    extraVolumes:
    - name: shared-data
      hostPath:
        path: /DATA
    - name: docker-storage
      hostPath:
        path: /var/run/docker.sock
    extraVolumeMounts:
    - name: shared-data
      mountPath: /srv/jupyterhub/data  # where hub can reach the shared data
    - name: docker-storage
      mountPath: /var/run/docker.sock
    extraConfig:
      myExtraConfig: |
        import os
        import git
        import shutil
        import tempfile 
        import subprocess
        import json
        import datetime
        import docker
        from repo2data.repo2data import Repo2Data
  
        async def my_pre_spawn_hook(spawner):
          repo_url = spawner.user_options.get('repo_url')
          repo_name = repo_url.split("/")[-1]
          user_name = repo_url.split("/")[-2]
          provider_name = repo_url.split("/")[-3]
          ref = spawner.user_options.get('image').split(':')[-1]  # commit hash
          # Create temporary dir
          t = tempfile.mkdtemp()
          # Clone into temporary dir
          print("Cloning {} and checkout to {}".format(repo_url, ref))
          repo = git.Repo.clone_from(repo_url, t, no_checkout=True)
          repo.git.checkout(ref)
          exists = os.path.isfile(os.path.join(t, 'binder/data_requirement.json'))
          if exists:
            # Copy desired file from temporary dir
            if os.path.isfile('data_requirement.json'):
              os.remove('data_requirement.json')
            shutil.move(os.path.join(t, 'binder/data_requirement.json'), '.')
            Repo2Data(server=True).install()
            # at this stage, automatically change data_dir download path to path to /DATA
            # this needs to happen on user environment pod (to update the notebooks) not here
            # regexp all paths inside the notebooks (.*?/projectName).*? (from repo2data req) and replace by DATA/projectName path relative to curr ipynb
            # https://stackoverflow.com/questions/24112727/relative-paths-based-on-file-location-instead-of-current-working-directory
            
          book_src_path = os.path.join(t, "content")
          book_dst_path = f"/srv/jupyterhub/data/book-artifacts/{user_name}/{provider_name}/{repo_name}/{ref}"
          built_flag_file = os.path.join(book_dst_path, "successfully_built")
          # check if repo is a jupyter book, if book artifacts already exists, and if user provided req file
          make_build = True
          if not os.path.isfile(os.path.join(book_src_path, "_config.yml")):
            print("{} not found".format(os.path.join(book_src_path, "_config.yml")))
            make_build = False
          if os.path.isfile(built_flag_file):
            print("{} exists, skipping book build".format(built_flag_file))
            make_build = False
          # final check, if commit message contains "--no-build-book", we don't build
          latest_message = repo.head.commit.message
          if re.match(".*?--no-build-book.*?", latest_message):
            print("--no-build-book flag present, jupyter book will not be built.")
            make_build = False
          if make_build:
            # initialize output book dir
            if not os.path.isdir(book_dst_path):
              os.makedirs(book_dst_path)
            os.chmod(book_dst_path, 0o777)
            # build book from within Docker
            book_dst_path_docker = book_dst_path.replace("srv/jupyterhub", "home/jovyan")
            docker_img = spawner.user_options.get('image')
            #client = docker.DockerClient(base_url='unix://var/run/docker.sock')
            #book_mount = docker.types.Mount(source="/DATA", target="/home/jovyan/data")
            #docker_cmd = f"jupyter-book build content --path-output {book_dst_path_docker}"
            #client.containers.run(docker_img, command=docker_cmd, mounts=book_mount)
            cmd = ['docker', 'run', '-v', '/DATA:/home/jovyan/data', docker_img, 'jupyter-book', 'build', '--path-output', book_dst_path_docker, 'content']
            print("Running jupyter-book build :\n {}".format(" ".join(cmd)))
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            print(process.stdout.read().decode("utf-8"))
            open(built_flag_file, 'w').close()
            # taring build book
            cmd = ["tar", "-zcvf", book_dst_path + ".tar.gz", book_dst_path] 
            print("Taring :\n {}".format(" ".join(cmd)))
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            print(process.stdout.read().decode("utf-8"))
    
          # Remove remaining stuff
          if os.path.isfile("data_requirement.json"):
            os.remove("data_requirement.json")
          shutil.rmtree(t)
  
        c.KubeSpawner.pre_spawn_hook = my_pre_spawn_hook
  proxy:
    service:
      type: NodePort
  cull:
    timeout: 1800 #30min
    every: 30
    max_age: 28800 #8h
    concurrency: 5 #to avoid Hub slow down, 5 concurrent processes
  singleuser:
    storage:
      type: none
      extraVolumes:
      - name: shared-data
        hostPath:
          path: /DATA
      extraVolumeMounts:
      - name: shared-data
        mountPath: /home/jovyan/data  # where each user can reach shared data
        readOnly : true
    memory:
       guarantee: 2G
    cpu:
       guarantee: 1

# BinderHub config
config:
  GitHubRepoProvider:
    banned_specs:
            #- ^(?!neurolibre\/.*).*
      - ^ines/spacy-binder.*
      - ^soft4voip/rak.*
      - ^hmharshit/cn-ait.*
      - ^shishirchoudharygic/mltraining.*
      - ^hmharshit/mltraining.*
  BinderHub:
    hub_url: https://binder.conp.cloud/jupyter
    use_registry: true
    image_prefix: conpdev/binder-

service:
  type: NodePort

storage:
  capacity: 2G

ingress:
  enabled: true
  hosts:
    - binder.conp.cloud
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
    cert-manager.io/issuer: letsencrypt-production
  https:
    enabled: true
    type: nginx
  config:
    # Allow POSTs of upto 64MB, for large notebook support.
    proxy-body-size: 64m
  tls:
    - secretName: binder-conp-cloud-tls
      hosts: 
        - binder.conp.cloud
